{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Unfair Bias in Machine Learning\n",
    "\n",
    "**[Code@Think UKI](https://www.ibm.com/uk-en/events/think-summit/codeatthink.html) workshop - 7 October 2020 -  Margriet Groenendijk** \n",
    "\n",
    "*Code from [these examples](https://github.com/Trusted-AI/AIF360/tree/master/examples) is used and adapted.*\n",
    "\n",
    "## Outline\n",
    "\n",
    "AI can embed human and societal bias and be then deployed at scale. \n",
    "\n",
    "Many algorithms are now being reexamined due to illegal bias. \n",
    "\n",
    "So how do you remove bias & discrimination in the machine learning pipeline? \n",
    "\n",
    "In this workshop debiasing techniques will be explored that can be implemented by using the open source toolkit [AI Fairness 360](https://github.com/IBM/AIF360). \n",
    "\n",
    "[1. Introduction](#intro)\n",
    "\n",
    "[2. Bias definitions](#definitions)\n",
    "\n",
    "* [2.1 Bias and variance](#stats)\n",
    "* [2.2 Statistical vs. cognitive bias](#defs)\n",
    "\n",
    "[3. AI fairness metrics](#metrics)\n",
    "\n",
    "* [3.1 Install aif360 and import packages](#install)\n",
    "* [3.2 Exploring data](#explore)\n",
    "* [3.3 Exploring bias](#bias)\n",
    "\n",
    "[4. Model building](#model)\n",
    " \n",
    "* [4.1 Train on the original data](#original) \n",
    " \n",
    "[5. AI fairness algorithms](#algorithms)\n",
    "\n",
    "* [5.1 Pre-processing](#preproc)\n",
    "* [5.2 In-processing](#inproc)\n",
    "* [5.3 Post-processing](#postproc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"intro\"></a>\n",
    "# 1. Introduction\n",
    "\n",
    "**In the UK this happened...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/MargrietGroenendijk/gitbooks2/blob/master/files/Alevels.png?raw=true\" width=\"1000\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This model was used to estimate students grades\n",
    "* Based on their previous grades, the school they went, the average for the school, etc.\n",
    "* Impacting their chance of getting accepted by universities\n",
    "\n",
    "# Is this fair?\n",
    "\n",
    "# How can you avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about bias\n",
    "\n",
    "<span style=\"font-family:Comic Sans MS\">Going back now to the start of my [PhD research](https://research.vu.nl/en/publications/boxing-nature-global-generalities-in-terrestrial-ecosystem-photos), I clearly knew then there was a big risk in using these observations to explore global scale relationships as I found in one of the first slides I made. But looking back now, I am not so sure I fully explored this enough. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/MargrietGroenendijk/gitbooks2/blob/master/files/points.png?raw=true\"  width=\"1000\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Emb(race)](https://www.ibm.org/responsibility/2019/case-studies/embrace)\n",
    "\n",
    "> IBM and IBMers stand with the Black community and call for change to ensure racial equality.\n",
    "\n",
    "How is this all connected? Can technology be used, even if it can only be a small part of a possible solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore fairness, and how to define and reduce bias in data and models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"definitions\"></a>\n",
    "# 2. Bias definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"stats\"></a>\n",
    "## 2.1 Bias and variance\n",
    "\n",
    "Some definitions of how I used to think of bias (and are the first ones coming up in a google search):\n",
    "\n",
    "The [**bias error**](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "\n",
    "The [**bias**](https://en.wikipedia.org/wiki/Bias_of_an_estimator) (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated.\n",
    "\n",
    "[**Statistical bias**](https://en.wikipedia.org/wiki/Bias_(statistics) is a feature of a statistical technique or of its results whereby the expected value of the results differs from the true underlying quantitative parameter being estimated.\n",
    "\n",
    "Probably clearer in this form with an example from [here](https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/):\n",
    "\n",
    "Error = Variance + Bias + Noise\n",
    "\n",
    "* The bias is a measure of how close the model can capture the mapping function between inputs and outputs.\n",
    "* The variance of the model is the amount the performance of the model changes when it is fit on different training data.\n",
    "* Bias-variance trade-off: reducing bias can easily be achieved by increasing variance, and the other way around\n",
    "\n",
    "This is not the bias that we will discuss in this workshop.\n",
    "\n",
    "> <span style=\"font-family:Comic Sans MS\">The bias rabbit hole... </span> links to links to even more definitions: [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias), [calibration bias](https://onlinelibrary.wiley.com/doi/10.1002/9781118736890.ch7) or [precision bias](https://en.wikipedia.org/wiki/Precision_bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"defs\"></a>\n",
    "## 2.2 Statistical vs. cognitive bias\n",
    "\n",
    "There are clearly a lot of definitions. Have these anything to do with fairness? Some of the definitions from [this page](https://en.wikipedia.org/wiki/Bias_(statistics)) are are types of bias that can cause unfairness:\n",
    "\n",
    "* **Selection bias** involves individuals being more likely to be selected for study than others, biasing the sample\n",
    "* **Spectrum bias** arises from evaluating diagnostic tests on biased patient samples, leading to an overestimate of the sensitivity and specificity of the test\n",
    "* The **bias of an estimator** is the difference between an estimator's expected value and the true value of the parameter being estimated\n",
    "* **Omitted-variable bias** is the bias that appears in estimates of parameters in regression analysis when the assumed specification omits an independent variable that should be in the model\n",
    "* **Detection bias** occurs when a phenomenon is more likely to be observed for a particular set of study subjects. \n",
    "* **Funding bias** may lead to the selection of outcomes, test samples, or test procedures that favor a study's financial sponsor.\n",
    "* **Reporting bias** involves a skew in the availability of data, such that observations of a certain kind are more likely to be reported.\n",
    "* **Analytical bias** arises due to the way that the results are evaluated.\n",
    "* **Exclusion bias** arise due to the systematic exclusion of certain individuals from the study.\n",
    "* **Attrition bias** arises due to a loss of participants e.g. loss to follow up during a study.\n",
    "* **Recall bias** arises due to differences in the accuracy or completeness of participant recollections of past events. e.g. a patient cannot recall how many cigarettes they smoked last week exactly, leading to over-estimation or under-estimation.\n",
    "* **Observer bias** arises when the researcher subconsciously influences the experiment due to cognitive bias where judgment may alter how an experiment is carried out / how results are recorded.\n",
    "\n",
    "For fair models, metrics are needed that can help explore if any of the above biases is present in data and models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"metrics\"></a>\n",
    "# 3. AI fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"install\"></a>\n",
    "## 3.1 Install aif360  and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360\n",
    "!pip install cvxpy\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../\")  \n",
    "\n",
    "# data exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# aif360 data, metrics and algorithms\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_german\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions import get_distortion_german\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"explore\"></a>\n",
    "## 3.2 Exploring data\n",
    "\n",
    "A [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29) that classifies people described by a set of attributes as good or bad credit risks.\n",
    "\n",
    "This data is one of the example [datasets](https://aif360.readthedocs.io/en/latest/modules/datasets.html#module-aif360.datasets) used in aif360 and has it's own [class](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.GermanDataset.html#aif360.datasets.GermanDataset) that will be used. \n",
    "\n",
    "### Load data\n",
    "\n",
    "It is assumed that the dataset can be found within a specific location. Let's create this folder and the download the data to this new folder. The advantage is that this works both locally and when running in Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aif360_location = !python -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\"\n",
    "import os\n",
    "install_loc = os.path.join(aif360_location[0], \"aif360/data/raw/german/\")\n",
    "%cd $install_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget ftp://ftp.ics.uci.edu/pub/machine-learning-databases/statlog/german/german.data\n",
    "!wget ftp://ftp.ics.uci.edu/pub/machine-learning-databases/statlog/german/german.doc\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_german = GermanDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIF360 data format\n",
    "\n",
    "All variables of this dataset are described in the [documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.GermanDataset.html) with more details in the description of the [`StandardDataset`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StandardDataset.html). In short, the dataset class contains a numpy array or pandas DataFrame with several variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_german.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'labels: {dataset_german.label_names}')\n",
    "print(f'protected attributes: {dataset_german.protected_attribute_names}')\n",
    "print(f'number of features: {len(dataset_german.feature_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore with pandas\n",
    "\n",
    "Convert the data to a `features` DataFrame and `labels` Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(dataset_german.features, columns=dataset_german.feature_names)\n",
    "labels = pd.Series(dataset_german.labels.ravel(), name=dataset_german.label_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe().transpose().head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [data description](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29):\n",
    "\n",
    "Attribute 1: (qualitative) \\\n",
    "Status of existing checking account \\\n",
    "A11 : ... < 0 DM \\\n",
    "A12 : 0 <= ... < 200 DM \\\n",
    "A13 : ... >= 200 DM / salary assignments for at least 1 year \\\n",
    "A14 : no checking account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distribution of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (18,18)\n",
    "\n",
    "features.hist();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features are binary, but a few are continuous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[['credit_amount','month','number_of_credits']]. \\\n",
    "        plot(subplots=True, \\\n",
    "             kind='hist', \\\n",
    "             layout=(2, 2),\n",
    "             sharex=False, \\\n",
    "             figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"bias\"></a>\n",
    "## 3.3 Exploring bias (the aif360 way)\n",
    "\n",
    "Let's review the basics of how a model is created in a supervised machine learning process to understand how bias can enter a machine learning model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/images/Complex_NoProc_V3.jpg\"   width=\"500\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias can enter the system in any of these three steps:\n",
    "1. The process starts with a training dataset, which contains a sequence of instances, where each instance has two components: the features and the correct prediction for those features. \n",
    "2. A machine learning algorithm is trained on this training dataset to produce a machine learning model. This generated model can be used to make a prediction when given a new instance. \n",
    "3. A second dataset with features and correct predictions, called a test dataset, is used to assess the accuracy of the model. Since this test dataset is the same format as the training dataset, a set of instances of features and prediction pairs, often these two datasets derive from the same initial dataset. A random partitioning algorithm is used to split the initial dataset into training and test datasets.\n",
    "\n",
    "* The training data set may be biased in that its outcomes may be biased towards particular kinds of instances\n",
    "* The algorithm that creates the model may be biased in that it may generate models that are weighted towards particular features in the input\n",
    "* The test data set may be biased in that it has expectations on correct answers that may be biased\n",
    "\n",
    "These three points in the machine learning process represent points for testing and mitigating bias. In AI Fairness 360 these are called:\n",
    "\n",
    "* pre-processing \n",
    "* in-processing\n",
    "* post-processing\n",
    "\n",
    "### Bias in a credit dataset\n",
    "\n",
    "Bias could occur based on age or sex in this dataset. \n",
    "\n",
    "* set the protected attribute to be `age`, where `age >=25` is considered privileged\n",
    "* the protected attribute for `sex` is not consider in this evaluation\n",
    "* split the original dataset into training and testing datasets\n",
    "* set two variables for the privileged (1) and unprivileged (0) values for the age attribute. These are key inputs for detecting and mitigating bias\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    " <b>OPTIONAL EXERCISE</b> <br/> \n",
    " To explore the gender bias in this dataset, edit the below code to use `sex` as the protected attribute and assign new privileged and unprivileged groups.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics based on a single `BinaryLabelDataset`\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "<b>Read <a href=\"https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html\">the documentation</a> for a full overview of this class and a list of all bias metrics. <a href=\"http://aif360.mybluemix.net/data\">This demo</a> provides definitions of the metrics as well.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_german = GermanDataset(protected_attribute_names=['age'],\n",
    "                    privileged_classes=[lambda x: x >= 25],      \n",
    "                    features_to_drop=['personal_status', 'sex']) \n",
    "\n",
    "# Split into train, validation, and test\n",
    "dataset_german_train, dataset_german_test = dataset_german.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_german_train = BinaryLabelDatasetMetric(dataset_german_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "metric_german_test = BinaryLabelDatasetMetric(dataset_german_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(metric_german_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/MargrietGroenendijk/gitbooks2/blob/master/files/metrics.png?raw=true\" width=\"1000\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `mean_difference`: alias of `statistical_parity_difference` \n",
    "    * Difference of the rate of favorable outcomes received by the unprivileged group to the privileged group. \n",
    "    * A negative value indicates less favorable outcomes for the unprivileged groups\n",
    "    * The ideal value of this metric is 0\n",
    "    * Fairness for this metric is between -0.1 and 0.1\n",
    "    \n",
    "\n",
    "* `disparate_impact`: ratio of rate of favorable outcome for the unprivileged group to that of the privileged group\n",
    "\n",
    "    $\\frac{Pr(Y = 1 | D = \\text{unprivileged})}\n",
    "     |         {Pr(Y = 1 | D = \\text{privileged})}$\n",
    "     \n",
    "     \n",
    "* `consistency`: individual fairness metric that measures how similar the labels are for similar instances.\n",
    "\n",
    "    $1 - \\frac{1}{n\\cdot\\text{n_neighbors}}\\sum_{i=1}^n |\\hat{y}_i -\n",
    "     |         \\sum_{j\\in\\mathcal{N}_{\\text{n_neighbors}}(x_i)} \\hat{y}_j|$\n",
    "\n",
    "\n",
    "* `base_rate`\n",
    "\n",
    "    $Pr(Y = 1) = P/(P+N)$       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"mean_difference = %f\" % metric_german_train.mean_difference())\n",
    "print(\"disparate_impact = %f\" % metric_german_train.disparate_impact())\n",
    "print(\"consistency = %f\" % metric_german_train.consistency())\n",
    "print(\"base_rate = %f\" % metric_german_train.base_rate())\n",
    "print(\"num_negatives = %f\" % metric_german_train.num_negatives())\n",
    "print(\"num_positives = %f\" % metric_german_train.num_positives())\n",
    "print(\"smoothed_empirical_differential_fairness = %f\" % metric_german_train.smoothed_empirical_differential_fairness())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"font-size:200%\">\n",
    "<b>Question / discussion time</b> <br> \n",
    "</div>\n",
    "\n",
    "* How would you visualise these metrics?\n",
    "* Do you think these are meaningful?\n",
    "* What if you are looking at multiple classes or a regression problem?\n",
    "* Or where do you start when you cannot easily define the privileged and unprivileged groups?    \n",
    "    \n",
    "<div class=\"alert alert-success\">\n",
    " <b>OPTIONAL EXERCISE</b> <br/> \n",
    " To explore the gender bias in this dataset, edit the below code to use `sex` as the protected attribute and assign new privileged and unprivileged groups.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"model\"></a>\n",
    "## 4. Model building\n",
    "\n",
    "### Binary Classification\n",
    "\n",
    "Some of the model options:\n",
    "* Logistic regression\n",
    "* Decision trees\n",
    "* Random forests\n",
    "* Bayesian networks\n",
    "* Support vector machines\n",
    "* Neural networks\n",
    "\n",
    "### But first: scale and normalise features\n",
    "\n",
    "* tidy dataset, so this is going to be unreaslistically easy, e.g. there are no missing values\n",
    "* one-hot encoding for multiple classes (already done, e.g., [features A11-A14](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29))\n",
    "* features need to be standardised, from same distribution\n",
    "\n",
    "[StandardScaler](https://scikit-learn.org/stable/modules/preprocessing.html) - \n",
    "*Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. `StandardScaler` implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set.*\n",
    "\n",
    "aif360 format can be used with scikitlearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_german = StandardScaler().fit(dataset_german_train.features)\n",
    "\n",
    "X_train = scale_german.transform(dataset_german_train.features)\n",
    "y_train = dataset_german_train.labels.ravel()\n",
    "w_train = dataset_german_train.instance_weights.ravel()\n",
    "\n",
    "X_test = scale_german.transform(dataset_german_test.features)\n",
    "y_test = dataset_german_test.labels.ravel()\n",
    "w_test = dataset_german_test.instance_weights.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the data look like now?\n",
    "plt.rcParams[\"figure.figsize\"] = (18,18)\n",
    "\n",
    "scaled_features = pd.DataFrame(X_train, columns=dataset_german.feature_names)\n",
    "\n",
    "scaled_features.hist();\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "<b>If you are new to scikit-learn read this <a href=\"https://developer.ibm.com/series/learning-path-machine-learning-for-developers/\">practical introduction</a> for a quick overview.<br>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "<a class=\"anchor\" id=\"original\"></a>\n",
    "### 4.1 Train on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "\n",
    "# create an instance of the model\n",
    "lmod = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "lmod.fit(X_train, y_train, \n",
    "         sample_weight=dataset_german_train.instance_weights)\n",
    "\n",
    "# calculate predicted labels\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# assign positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_german_train.favorable_label)[0][0]\n",
    "\n",
    "# add predicted labels to predictions dataset\n",
    "dataset_german_train_pred = dataset_german_train.copy()\n",
    "dataset_german_train_pred.labels = y_train_pred\n",
    "\n",
    "dataset_german_test_pred = dataset_german_test.copy(deepcopy=True)\n",
    "X_test = scale_german.transform(dataset_german_test_pred.features)\n",
    "y_test = dataset_german_test_pred.labels\n",
    "dataset_german_test_pred.scores = lmod.predict_proba(X_test)[:,pos_ind].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "score = lmod.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, lmod.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "[fig, ax] = plt.subplots(1, figsize=(5, 5));\n",
    "plot_confusion_matrix(lmod, X_test, y_test,\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      normalize='true',ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "metrics.plot_roc_curve(lmod, X_test, y_test);                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(dataset_german_test.features, columns=dataset_german_test.feature_names)\n",
    "\n",
    "[fig, ax] = plt.subplots(1,2, figsize=(15, 5));\n",
    "plot_confusion_matrix(lmod, X_test[df_test['age']==0], y_test[df_test['age']==0],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[0]);\n",
    "ax[0].set_title('Age < 25')\n",
    "\n",
    "plot_confusion_matrix(lmod, X_test[df_test['age']==1], y_test[df_test['age']==1],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[1]);\n",
    "ax[1].set_title('Age > 25');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"algorithms\"></a>\n",
    "## 5. AI fairness algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/MargrietGroenendijk/gitbooks2/blob/master/files/pipeline.png?raw=true\" width=\"1000\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/MargrietGroenendijk/gitbooks2/blob/master/files/algorithms.png?raw=true\"  width=\"1000\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"preproc\"></a>\n",
    "### 5.1 pre-processing algorithms\n",
    "\n",
    "### Remove bias by reweighing data\n",
    "\n",
    "**Reweighing** is a preprocessing technique that weights the examples in each (group, label) combination differently to ensure fairness before classification.\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "<b>Read the <a href=\"https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html\">aif360 documentation</a> for a full overview<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "               privileged_groups=privileged_groups)\n",
    "\n",
    "# compute the weights for reweighing the dataset\n",
    "RW.fit(dataset_german_train)\n",
    "\n",
    "# transform the dataset to a new dataset based on the estimated transformation\n",
    "dataset_rw_train = RW.transform(dataset_german_train)\n",
    "dataset_rw_test = RW.transform(dataset_german_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"mean_difference = %f\" % metric_german_train.mean_difference())\n",
    "print(\"disparate_impact = %f\" % metric_german_train.disparate_impact())\n",
    "print(\"consistency = %f\" % metric_german_train.consistency())\n",
    "print(\"base_rate = %f\" % metric_german_train.base_rate())\n",
    "print(\"num_negatives = %f\" % metric_german_train.num_negatives())\n",
    "print(\"num_positives = %f\" % metric_german_train.num_positives())\n",
    "print(\"smoothed_empirical_differential_fairness = %f\" % metric_german_train.smoothed_empirical_differential_fairness())\n",
    "\n",
    "metric_rw_train = BinaryLabelDatasetMetric(dataset_rw_train, \n",
    "                                         unprivileged_groups=unprivileged_groups,\n",
    "                                         privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Reweighted training dataset\"))\n",
    "print(\"mean_difference = %f\" % metric_rw_train.mean_difference())\n",
    "print(\"disparate_impact = %f\" % metric_rw_train.disparate_impact())\n",
    "print(\"consistency = %f\" % metric_rw_train.consistency())\n",
    "print(\"base_rate = %f\" % metric_rw_train.base_rate())\n",
    "print(\"num_negatives = %f\" % metric_rw_train.num_negatives())\n",
    "print(\"num_positives = %f\" % metric_rw_train.num_positives())\n",
    "print(\"smoothed_empirical_differential_fairness = %f\" % metric_rw_train.smoothed_empirical_differential_fairness())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on reweighted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scale_rw = StandardScaler().fit(dataset_rw_train.features)\n",
    "\n",
    "X_train_rw = scale_rw.transform(dataset_rw_train.features)\n",
    "y_train_rw = dataset_rw_train.labels.ravel()\n",
    "w_train_rw = dataset_rw_train.instance_weights.ravel()\n",
    "\n",
    "X_test_rw = scale_rw.transform(dataset_rw_test.features)\n",
    "y_test_rw = dataset_rw_test.labels.ravel()\n",
    "w_test_rw = dataset_rw_test.instance_weights.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rw_train.instance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new instance of the model\n",
    "lmod_rw = LogisticRegression()\n",
    "\n",
    "# train the model\n",
    "lmod_rw.fit(X_train_rw, y_train_rw, \n",
    "         sample_weight=dataset_rw_train.instance_weights)\n",
    "\n",
    "# calculate predicted labels\n",
    "y_train_pred_rw = lmod_rw.predict(X_train_rw)\n",
    "\n",
    "# assign positive class index\n",
    "pos_ind_rw = np.where(lmod_rw.classes_ == dataset_rw_train.favorable_label)[0][0]\n",
    "\n",
    "# add predicted labels to predictions dataset\n",
    "dataset_rw_train_pred = dataset_rw_train.copy()\n",
    "dataset_rw_train_pred.labels = y_train_pred_rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "print(score)\n",
    "score_rw = lmod_rw.score(X_test_rw, y_test_rw)\n",
    "print(score_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "metrics.plot_roc_curve(lmod_rw, X_test_rw, y_test_rw); \n",
    "plt.title('Reweighted model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "metrics.plot_roc_curve(lmod, X_test, y_test); \n",
    "plt.title('Original model');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm_rw = metrics.confusion_matrix(y_test_rw, lmod_rw.predict(X_test_rw))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm_rw, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Reweighted model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fig, ax] = plt.subplots(2,2, figsize=(15, 12));\n",
    "plot_confusion_matrix(lmod, X_test[df_test['age']==0], y_test[df_test['age']==0],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[0,0]);\n",
    "ax[0,0].set_title('Age < 25')\n",
    "\n",
    "plot_confusion_matrix(lmod, X_test[df_test['age']==1], y_test[df_test['age']==1],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[0,1]);\n",
    "ax[0,1].set_title('Age > 25');\n",
    "\n",
    "plot_confusion_matrix(lmod_rw, X_test_rw[df_test['age']==0], y_test_rw[df_test['age']==0],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[1,0]);\n",
    "ax[1,0].set_title('Age < 25 (Reweighted)')\n",
    "\n",
    "plot_confusion_matrix(lmod_rw, X_test_rw[df_test['age']==1], y_test_rw[df_test['age']==1],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[1,1]);\n",
    "ax[1,1].set_title('Age > 25 (Reweighted)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fig, ax] = plt.subplots(2,2, figsize=(15, 12));\n",
    "plot_confusion_matrix(lmod, X_test[df_test['age']==0], y_test[df_test['age']==0],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[0,0],normalize='true');\n",
    "ax[0,0].set_title('Age < 25')\n",
    "\n",
    "plot_confusion_matrix(lmod, X_test[df_test['age']==1], y_test[df_test['age']==1],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[0,1],normalize='true');\n",
    "ax[0,1].set_title('Age > 25');\n",
    "\n",
    "plot_confusion_matrix(lmod_rw, X_test_rw[df_test['age']==0], y_test_rw[df_test['age']==0],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[1,0],normalize='true');\n",
    "ax[1,0].set_title('Age < 25 (Reweighted)')\n",
    "\n",
    "plot_confusion_matrix(lmod_rw, X_test_rw[df_test['age']==1], y_test_rw[df_test['age']==1],\n",
    "                      cmap=plt.cm.Blues, \n",
    "                      display_labels=['good credit','bad credit'],\n",
    "                      ax=ax[1,1],normalize='true');\n",
    "ax[1,1].set_title('Age > 25 (Reweighted)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bias with the optimized data pre-processing algorithm \n",
    "\n",
    "\n",
    "The debiasing function used is implemented in the [OptimPreproc](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.OptimPreproc.html?highlight=get%20distortion) class. It modifies training data features & labels.\n",
    "\n",
    "* Define parameters for optimized pre-processing specific to the dataset.\n",
    "* Divide the dataset into training, validation, and testing partitions.\n",
    "* Learn the optimized pre-processing transformation from the training data.\n",
    "* Train classifier on original training data.\n",
    "* Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints (from the original validation set).\n",
    "* Determine the prediction scores for original testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "* Transform the testing set using the learned probabilistic transformation.\n",
    "* Determine the prediction scores for transformed testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "\n",
    "See the example notebook here: https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_optim_data_preproc.ipynb\n",
    "\n",
    "#### Train with and transform the original training data\n",
    "\n",
    "This algorithm does not use the privileged and unprivileged groups that are specified during initialization yet. Instead, it automatically attempts to reduce statistical parity difference between all possible combinations of groups in the dataset.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    " <b>This seems to take very long to run, you might want to skip this.</b> <br/> \n",
    " The algorithm does not use the privileged and unprivileged groups that are specified during initialization yet. Instead, it automatically attempts to reduce statistical parity difference between all possible combinations of groups in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_options = {\n",
    "            \"distortion_fun\": get_distortion_german,\n",
    "            \"epsilon\": 0.1,\n",
    "            \"clist\": [0.99, 1.99, 2.99],\n",
    "            \"dlist\": [.1, 0.05, 0]\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OP = OptimPreproc(OptTools, optim_options,\n",
    "#                  unprivileged_groups = unprivileged_groups,\n",
    "#                  privileged_groups = privileged_groups)\n",
    "\n",
    "#OP = OP.fit(dataset_german_train)\n",
    "\n",
    "# Transform training data and align features\n",
    "#dataset_op_train = OP.transform(dataset_german_train, transform_Y=True)\n",
    "#dataset_op_train = dataset_german_train.align_datasets(dataset_transf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric with the transformed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric_op_train = BinaryLabelDatasetMetric(dataset_op_train, \n",
    "#                                         unprivileged_groups=unprivileged_groups,\n",
    "#                                         privileged_groups=privileged_groups)\n",
    "\n",
    "#display(Markdown(\"#### Optimized training dataset\"))\n",
    "#print(\"mean_difference = %f\" % metric_op_train.mean_difference())\n",
    "#print(\"disparate_impact = %f\" % metric_op_train.disparate_impact())\n",
    "#print(\"consistency = %f\" % metric_op_train.consistency())\n",
    "#print(\"base_rate = %f\" % metric_op_train.base_rate())\n",
    "#print(\"num_negatives = %f\" % metric_op_train.num_negatives())\n",
    "#print(\"num_positives = %f\" % metric_op_train.num_positives())\n",
    "#print(\"smoothed_empirical_differential_fairness = %f\" % metric_op_train.smoothed_empirical_differential_fairness())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load, clean up original test data and compute metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_orig_test = dataset_op_train.align_datasets(dataset_orig_test)\n",
    "#display(Markdown(\"#### Testing Dataset shape\"))\n",
    "#print(dataset_orig_test.features.shape)\n",
    "\n",
    "#metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "#                                         unprivileged_groups=unprivileged_groups,\n",
    "#                                         privileged_groups=privileged_groups)\n",
    "#display(Markdown(\"#### Original test dataset\"))\n",
    "#print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"inproc\"></a>\n",
    "### 5.2 In-processing algorithms\n",
    "\n",
    "* [Adversarial Debiasing](https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_adversarial_debiasing.ipynb) - Uses adversarial techniques to maximize accuracy & reduce evidence of protected attributes in predictions\n",
    "* [Reject Option Classification](https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_reject_option_classification.ipynb) - Adds a discrimination-aware regularization term to the learning objective\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"postproc\"></a>\n",
    "### 5.3 Post-processing algorithms\n",
    "\n",
    "[Reject option classification (ROC)](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html?highlight=reject) is a postprocessing technique that gives favorable outcomes to unpriviliged groups and unfavorable outcomes to priviliged groups in a confidence band around the decision boundary with the highest uncertainty.\n",
    "\n",
    "* The debiasing function used is implemented in the RejectOptionClassification class.\n",
    "* Divide the dataset into training, validation, and testing partitions.\n",
    "* Train classifier on original training data.\n",
    "* Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints.\n",
    "* Estimate the optimal classification threshold, and the critical region boundary (ROC margin) using a validation set for the desired constraint on fairness. The best parameters are those that maximize the classification threshold while satisfying the fairness constraints.\n",
    "* The constraints can be used on the following fairness measures:\n",
    "    * Statistical parity difference on the predictions of the classifier\n",
    "    * Average odds difference for the classifier\n",
    "    * Equal opportunity difference for the classifier\n",
    "* Determine the prediction scores for testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "* Using the determined optimal classification threshold and the ROC margin, adjust the predictions. Report accuracy and fairness metric on the new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = dataset_german_test_pred.scores > class_thresh\n",
    "    dataset_german_test_pred.labels[fav_inds] = dataset_german_test_pred.favorable_label\n",
    "    dataset_german_test_pred.labels[~fav_inds] = dataset_german_test_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_german_test = ClassificationMetric(dataset_german_test,\n",
    "                                             dataset_german_test_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    \n",
    "    ba_arr[idx] = 0.5*(classified_metric_german_test.true_positive_rate()\\\n",
    "                       +classified_metric_german_test.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"Statistical parity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "\n",
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=metric_name,\n",
    "                                  metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(dataset_german_test, dataset_german_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAAVCAYAAABCBUm8AAAMYklEQVR4Ae2bB6w1RRmGKaJUUVAQUFQUe0UTu4IiRQ0BGyq2gL1jIjEGC6BCICCxIwIxxsSKxIJGsWLE3o1iAUsQ7Iq9PuZZ3rmZs2f3lP/e6w/5Z5Kb3TM7880379dn9262WWsNgYZAQ6Ah0BBoCDQEGgINgYZAQ6Ah0BBoCDQEGgINgYZAQ6Ah0BBoCDQE1hAB4PrAQ4EbFrLANunbvfTNugJ3Bk53DPBG4D65fw7wYeA2wPtcaxadRZ4B9wROXGTseowBTgHuDtzbva7VGsATgCNresDT/Kv76vt5z3tjV4Ub8AzgmTXNq/P9MvyuVparnb8ojhtb9xflc5lxwLbAucC1l5m3VmM3xJ7XS95A8Ze7De1vGZ0emr9UXzYJ8OYyEXiBHcBjS9+sK/BA4IeOAU4A9gGuBfwDeDxwY+kD151FZ+wZ8FTg5NA/BPja2Nj17gc+CxyRPR4/tl7N89iYuh84Hjiz13cacFrdV9/7bNbz3thV4ZY9P66mOXafZGOvsefr1Q+cAxwq/SX5VV9HZTnEby3febowNH9D+oBVyXDZNYHbA59bdt4y4/UJ8TUb5BuWWWto7IbY83rIu+cvdxzhVb+zkA0OzV+qrwoMOnEd+HWAyyKslcAA7A5sXRMHtgD2rANDeR46ktml9NVX4ObA5r2+vQSo7vMeOA44I/edcaSq2aMeC2wF3KTu699nj1Nr1ONGeNs1a3aBoR7vvRmPtEt/zXPVt8cAhtsDN1gkMAA7AbtW9LrAkP4Jw+pjsaxTicEOZi7Z7+bAhPMHtgx2HwSuKHyWa5//qt/9X6/87l+H5OGYviyBz4wlM5HPnoV2aG5Rfs+7AresdXNIvoVGsF9Zq+q3Ep/Q2eqZujuBp88cry3lfmndF1dg58x3jZuWNct1Br77AReXcaGxY6FX95f7RXCusZwVGPo6XNaor30dqJ+V+6H9aUerteeKvrgOyXvIRgyEU3Y15C/1ZcD2ZZ1516F9zpsz+jyB4ZfAj4HXAs8GrgS+q5EBO+QYSCf/e+AoiQF3BX4E/CdzS8XwEeCgRGLn/Ba4B3B5aJmFfAP4L/C90NnZKiAVxhUeYxWGgUcD/06gem8cnHOlZzs2/Dyi6pOHnQqNPLfP9aT1U+Be4ecvwIeAbyVDmuAtc8XFeb8C/pBsVKP5ZJ6L06+BfwHn51io5llleH+wEo8jMs/joz8CfwN+NqtiSMn71+BWqicDg0HcPfwZeHroTmExFBjiqH9RAjTw9RxXeGwkTyYLYq5DOzbVoPpg//fDywVZ06Mq9yAvthfbX9oI/1aX6pBz/g7sX8Z7BW4XnqT3A3lL/5AsX3fVsp0+Ht3jV1mIk3rzLuDC3H8lyU0ny+z9N4B/4vlN4A7ZqzTE6hYDOlnrwmEGxfDy8Tgg7eF3lX52Olv2mnW1QXFVBgYhj3jVS3VKfh61jO7HBsVVfRXb1wA/CV9dFTqm78HYYK0fsF2ePqta9UKe3iB21R7Ui3k4D2G5UjF4VAlcAtwKmNLhslZ4GdIBeXDP623P8+Q9ZiNTdlX21POX+ib9iHgqu8Oy59oGZ+6z79vKOgtfExhUZBVaJnRczwe+mMDw8jhynfdDwqyR7FPA23JG6PgSGHTwZjZbR6mMzLXwzep0CGaXOtyLMl4lvFEc9q3rDQCv6lUM8msmdHj4LQrxICMs8IX+e4gEHoGVrzcBOoXC17N8x5KMs8/bA4B/JoC55qUJDCV7c22N5cGhfVIUu+bZiueCZAUauDjvAujoXdtq4z1zAsMLU1ncJYZp9WBgUIGs8p4YPs0wVZoJLEYCg2NtnYHHcdw3WLkPsZRfM59SnRTMDARWizozeXon8JaMVaYv6clwjP8PZP+vB87rzVHH3ho+xLPLXodk6TyPPUqpPcCvyYlJie0xkbdB+f41NuH/ttGrJ2dvJicejRokT81atXyLLmyXgHIkYGatzM8I/b7Obln2Gl61C+cfGr18pYlHZKtuiZ/r9OkM6n41Vj7UMZ25JwIHRD8M9lO2WHjKHl334mCiHShrg7X2b7CqTxSKXszCWT2ZwLKyQW1Emr6/E4cpHe7xpp/ZWPY8T94Fi76NyPOEXZU99fyltu1xvjqnb/1q5NG3wVl+q/DQjSnrLHwtgSEL+xLoS8miSmB4d+1kYzA6nZ+XzL4+SorRCpzOyqYDLEx6NbN27pfdcJTffh2jz6xcDqg3AGgkE0dJ4desxmb1YpOmfwqgy6oLncJX5ukMzKRW+Er/EG9HSbOiU94xFOXQUMxEVww9tGqexdDAJ28GJPkz67Btl/Gj7xiiIGcmg3W+TafcKUrmG2g1/ofn+QQWtfOr9jIWGAy43wnP8u6RYV8pu6OrjDsYeF7k6scHfwIOrtZRwefx/0iTkTIne7JCOzD3d8y+pCV+h6S/k2XuDQyds5rBrzLeJ+MNYDqqTpbp2y0B8qX5vX+yOasHdebs9Nfy7etCd+QKPDfVSU2/6Ow20rEBd8o4A5VJjbIV9+MypIwbojOo+709GRCvzFq1XU7pe289916CsQ7movI8pwgnVL+HbKmP8xSWlQ0qXit6A5Y2ZZvQ4bKW1yEdqGgV3Zzanycea2DP8+Tdx0Jb0kam7KrsqecvDeB+tPPtVOGXZM9jNji0zwkeyjoLX3uBwYx97zBRAsMr4pBUaL9esryRcbO5tye6m830K4ZaAVeYTJZycpyNmZtf3pglWEbq3HyBeG69gQQGs1EdVG0cVjE2r2YYD8u5pFVO92VUoRNFelkUzxfhdcVQFMkMqs9bqRjulvLectyXQEU5zKqtGMwuVepTE6h0HIVnsyFp+wVG9wVXxloxeHQnVn61NfTy+fQ4Dvepw9j7qi1zszi/j2a+61vZlIphAovw6zFFXf6bmcm7WaSVncHrfh4DZR3XsOm4xpRS5VXpj8lxj9f9Cu7RJefbhvgvxxoeHfQDw6eBs1IxnFg5KQPDhCyzjoFB2QxVOEXGo4EhjsWjxLMTjNVHkyV/S1N9Pydr1fItuqB81cOnpKL9RFUxdB9MRFfFog4MHkUemKxR52iQlb46qt0Z/Mw0F9b93tixwDCl7z25dYEhdrlvKgZpKUdPFoYqhlk4T2FZOXN1rWBfKoYJHe7xNqUDFa3Cw9T+gLWw53nyXvF30RVtxNOWKbsqe+oFBuXvEav+Tl3yeG2WTg/tc4KHss7C1zjly/oTgM8n+7QU9UzWpuM52rHVeaz9loAlMBjlPVP1eMT3D1YMlrtm1TLrFyBWDGa3lsV+0WTWY6Vgqeo5bpclFp5S+ru2QcPgVBuZgUrH+qQczThOBZv41DaBwQzUZuZnxq4CymNRpCnestd3ZJ5O1D+dT82HX015PGQz4/PIyaBXePa3Tss9e3Z9TOiWEt95vr/pBwaPJJzjWh8LhmJn9qqR+elsedciDi8K3Sksclau4z+l4JqxOhz5NFu1GRg8r3c/Bq7zopSu5dg+Ziq9FeSrQ8dqQVmfX9bJ/CH+pXdS+DAwXFjmpM+jB99b2HTo+6ZfpzAhy/SX9wzuf4zfOjAYeDy66WQJWLXUTVnrnHw/IEZWi+JioKvlW+uC740MDrZOD2MPUzpb9ppjQHVS/Te7tGoxIFi925SbOlavYzI0qvu9NX1nUSoGX5SqU9rloL5XfPk+Rb28NPh63Fea72pWquQBvRjCeQhLj1uKb7BSEHMd75QOF77Cy5QODPAwuD9gtfZcy2FI3mM2MmVXZU89f2nibTBQvp6qqOsGizGdntpnH4uyzppf49S7ryMK8WQSK///UPrnXeMoJl4OOycOdeJLpXm06uepFjonX/eHtoqkwmlwo18lzeDNl4ET+6/XiGB3qPv698Fwq7o/VcQgz/U475NtrmT85XkCb3ckVfXpABala/DftszNWr6LWclq62dD90kODs9cX57ZJtYf43+IXukr8uhVOgvJstBY7TV6tfD/4GT86FdWQ/wkO5z6TDHvKkb1tdCapftlzNC14Dv0bKgvDmdC14bGjfUtg+WsPZWjpI1pz2WPy8g7VeFCdlX/b1lZa+y6rBzH6Gxy/TmvPGiT2/j/acP+g2Oyais/s8Wz1mvpJsv1QvaaQ7fpwDVHVo3TTRyBVC4eg2xw1beJQ9i23xBoCDQEGgINgYZAQ6Ah0BBoCDQEGgINgYZAQ6Ah0BBoCDQEGgINgYZAQ6Ah0BBoCDQEGgINgYZAQ6Ah0BBoCDQEGgINgWswAv8DCNazREdQnq8AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Odds Equalizing](https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_calibrated_eqodds_postprocessing.ipynb) - Modifies the predicted label using an optimization scheme to make predictions fairer\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "Another full example: https://nbviewer.jupyter.org/github/IBM/AIF360/blob/master/examples/tutorial_medical_expenditure.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
